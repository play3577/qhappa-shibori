qhapaqの学習部分です。leaner_ndf_parallel.hppが本体です。aperyの学習部を差し替えて使ってください。大樹の枝（https://github.com/HiraokaTakuya/apery/releases/tag/SDT3）のlearner.hppをすげ替えることで動作することを確認しています。最新のaperyについては動作しないかもわかりません。

学習方法：

１．棋譜と評価値のファイルを用意します
棋譜はsfen形式です。評価値には棋譜の各々の局面を何点にするように学習させたいかが記載されています。評価値0点の局面は学習からは外します。

script/autobattle-p2.py で生成した棋譜については、script/zoukinmaker-onlygood.pyで変換することが可能です。

２．学習させます
以下にインプットの一例を示します。
./apery l 0 0407out400-8.sfen 0407out400-8.txt dummy.sfen dummy.txt 4 1 66 33 8000 7500 0 32 14 0 1 700 -1 1 1 1 0.999 0.0000001 1 30

パラメタは前から順番に
・大樹時代の名残の置き字　l
・デバッグモード（評価値を更新させない） 0(false)
・学習させる棋譜（0407out400-8.sfen）
・学習させる評価値（0407out400-8.txt）
・学習結果を評価するための棋譜（dummy.sfen）
・学習結果を評価するための評価値（dummy.txt）
・スレッド数 4
・bestmove以外の手がbestmoveより点数が高かった時に加える補正項の割合 1
・静止探索の代わりにdepth2、depth3で読む確率（66,33でdepth1-3が等確率）
・過学習防止用に出現回数に依存した変調を勾配にかける。8000 7500なら、出現回数0-500回は学習に用いない、7500-15500回は0-1倍の変調をかける
・元の値が0だったパラメタは学習させない 0(false)
・重複した局面は最初に出た奴しか学習に使わない 1(true)
・評価値がこの値を超えたらその棋譜の学習を終わらせる 700
・movepickerの上位n手をボナメソ学習させる -1（全ての合法手を使う）
・交差エントロピーを使うか否か 1(true)
・adadeltaの学習パラメタ1 1 0.999 0.0000001
・評価値の更新頻度 1 iterationごと
・最大iteration回数 30

３．学習の理論的な特徴
各々の局面の評価値を何らかの目標値（深く読んだ評価値など）に近づける事自体はNDFのメソッド（雑巾絞り）と同じですが、河童絞りは「如何にして目標値を正確にするか」を強く意識しています。

具体的には「負けた方の評価値を使わない」「逆転模様の棋譜については最後の逆転が発生した後の評価値しか使わない」「現在の評価値と数手先の評価値の混ぜあわせた評価値を使うことで、徐々に良くなる展開や土下座する展開を考慮する」といった工夫をしています（詳しい数式はpythonスクリプトをご覧ください）

加えて、勝った側の手を指す確率を上げることを意図したボナメソ的な学習も組み入れられています。具体的には実際の局面で指された手以外の手を「評価値X-Y以下になる」（Xはbestmoveの評価値。Yはユーザが決める正の数。この辺の実装は技巧v1に詳しいです）ように調整しています。


４．ライブラリとしての利用について
本学習部はaperyベースとしています。そのため、大会などでの利用ルールはaperyに準拠するものとします。QhapaqもWCSCやsdtなどでライブラリ登録を行いますので、本ルーチンを使う場合は、Qhapaqのライブラリ申請も一緒にしていただけると幸いです。

５．謝辞
本メソッドはaperyのインフラに頼りきったメソッドです。aperyチームによるオープンソース化がなければ、河童絞りは世に姿を表すことはなかったでしょう。また、開発に際し多くのアドバイス、知見をくださった開発者の皆様（河童絞りは特に、やねうら王、たぬきのもり、技巧の影響を強く受けています）に感謝申し上げます。